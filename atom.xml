<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Quantified Savagery]]></title>
  <link href="http://candu.github.com/atom.xml" rel="self"/>
  <link href="http://candu.github.com/"/>
  <updated>2012-10-11T00:53:18-04:00</updated>
  <id>http://candu.github.com/</id>
  <author>
    <name><![CDATA[Evan Savage]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Self-Tracking For Panic: A Deeper Look]]></title>
    <link href="http://candu.github.com/blog/2012/10/09/self-tracking-for-panic-an-even-deeper-look/"/>
    <updated>2012-10-09T07:00:00-04:00</updated>
    <id>http://candu.github.com/blog/2012/10/09/self-tracking-for-panic-an-even-deeper-look</id>
    <content type="html"><![CDATA[<p>In this post, I apply three statistical and machine learning tools to my panic
recovery journal data: linear regression/correlation, the Fast Fourier
Transform, and maximum entropy modelling.</p>

<!-- more -->

<h2 id="first-a-word-about-tools">First, A Word About Tools</h2>

<blockquote><p>I suppose it is tempting, if the only tool you have is a hammer, to treat<br />everything as if it were a nail.</p><footer><strong>Abraham Maslow</strong> <cite>The Psychology of Science: A Reconnaissance</cite></footer></blockquote>

<h2 id="now-a-necessary-disclaimer">Now, A Necessary Disclaimer</h2>

<p>My experiment has fewer than 50 samples, which is <em>nowhere near enough to draw
statistically significant conclusions</em>. That’s not the point. The primary
purpose of this post is to <em>demonstrate analysis techniques by example</em>. These
same methods can be wielded on larger datasets, where they are much more
useful.</p>

<h2 id="getting-ready">Getting Ready</h2>

<p>To follow along with the examples here, you’ll need
the excellent Python toolkits
<a href="http://scipy.org/">scipy</a>,
<a href="http://matplotlib.org/">matplotlib</a>, and
<a href="http://nltk.org/">nltk</a>:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ pip install scipy nltk matplotlib</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="linear-regression">Linear Regression</h2>

<h3 id="what">What?</h3>

<p>Linear regression answers this question:</p>

<blockquote><p>What is the line that most closely fits this data?</p></blockquote>

<p>Given points $ P_i = (x_i, y_i) $, the goal is to find the line
$ y = mx + b $ such that some error function is minimized.
A common one is the least squares function:</p>

<script type="math/tex; mode=display">
f(m, b) = \sum_{i} \left(y_i - (mx_i + b)\right)^2
</script>

<p>The
<a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson correlation coefficient</a> $ R $ and
<a href="http://www.lstr.net/blog/2008/07/08/p-values-explained-well/">p-value</a> $ p $
are also useful here, as they measure <em>correlation</em> and <em>statistical
significance</em>.</p>

<h3 id="why">Why?</h3>

<p>In a self-tracking context, you might ask the following questions:</p>

<ul>
  <li>Have I been exercising more over time?</li>
  <li>Does exercise affect mood? By how much and in what direction?</li>
</ul>

<p>Linear regression can help address both questions. However, it can only find
<em>linear</em> relationships between datasets. Many dynamic processes are <em>locally linear</em>
but not <em>globally linear</em>. For instance, there are practical limits to how
much you can exercise in a day, so no linear model with non-zero slope will
accurately capture your exercise duration for all time.</p>

<h3 id="the-data">The Data</h3>

<p>You can see the code for this analysis <a href="https://github.com/candu/quantified-savagery-files/blob/master/Panic/recovery-journal/food_linregress.py">here</a>. I look at only the first
31 days, that being the largest consecutive run for which I have data.</p>

<p><img src="https://lh6.googleusercontent.com/-plD2webhfrY/UHXc4xHxAGI/AAAAAAAAACM/2X488DqHKms/s640/alcohol.jpg" title="Alcohol Consumption" /></p>

<p>My alcohol consumption did not decrease over time, but rather stayed fairly
constant: with $ R = 0.0098 $, there is no correlation between alcohol and time.</p>

<p><img src="https://lh5.googleusercontent.com/-UCZKlx5l5RI/UHXc6u8h2vI/AAAAAAAAACs/CWcJjS09dS8/s640/sweets.jpg" title="Sugar Consumption" /></p>

<p>Sugar consumption is a similar story: although the best-fit slope is slightly
negative, $ R = -0.0671 $ indicates no correlation over time. It seems that my
alcohol and sugar consumption were not modified significantly over the tracking
period.</p>

<p><img src="https://lh5.googleusercontent.com/-Ssz89uoE-EA/UHXc5DvHf0I/AAAAAAAAACU/o0C_PJpmZcM/s640/alcohol-and-sugar.jpg" title="Alcohol and Sugar Consumption" /></p>

<p>I decided to graph alcohol and sugar together. It looks like they might be
related, as the peaks in each seem to coincide on several occasions. Let’s
test this hypothesis:</p>

<p><img src="https://lh6.googleusercontent.com/-iCO9umA8L8s/UHXc5vImvhI/AAAAAAAAACc/d82SCqFs-qI/s640/alcohol-today-vs-yesterday.jpg" title="Alcohol vs. Sugar Consumption" /></p>

<p>The positive slope is more pronounced this time, but
$ R = 0.1624 $ still indicates a small degree of correlation. We can also look
at the p-value: with $ p = 0.3827 $, it is fairly easy to write this off as
a random effect.</p>

<p>Finally, let’s take another look at a question from
<a href="http://candu.github.com/blog/2012/10/08/self-tracking-for-panic-a-deeper-look/">a previous blog post</a>:</p>

<blockquote><p>On days where I drink heavily, do I drink less the day after?</p></blockquote>

<p><img src="https://lh6.googleusercontent.com/-iCO9umA8L8s/UHXc5vImvhI/AAAAAAAAACc/d82SCqFs-qI/s640/alcohol-today-vs-yesterday.jpg" title="Alcohol Consumption: Today vs. Yesterday" /></p>

<p>There’s a negative slope there, but the correlation and p-value statistics are
in the same uncertain zone as before. I likely need more data to investigate
these last two effects properly.</p>

<h2 id="fast-fourier-transform">Fast Fourier Transform</h2>

<h3 id="what-1">What?</h3>

<p>Fourier analysis answers this question:</p>

<blockquote><p>What frequencies comprise this signal?</p></blockquote>

<p>Given a sequence $ x_n $, a
<a href="http://en.wikipedia.org/wiki/Discrete_Fourier_transform">Discrete Fourier Transform</a> (DFT)
computes</p>

<script type="math/tex; mode=display">
X_k = \sum_{n=0}^{N-1} x_n \cdot e^{\frac{-2 i \pi k n}{N}}
</script>

<p>The $ X_k $ encode the amplitude and phase of frequencies
$ \frac{f k}{N} $ Hz, where $ T $ is the time between samples
and $ f = 1 / T $ is the sampling frequency.</p>

<p>As described here, the DFT requires $ \mathcal{O}(N^2) $ time to
compute. The <a href="http://mathworld.wolfram.com/FastFourierTransform.html">Fast Fourier Transform</a> (FFT) uses
divide-and-conquer on this sum of complex exponentials to compute the DFT in
$ \mathcal{O}(N \log N) $ time. 
<a href="http://groups.csail.mit.edu/netmit/sFFT/">Further speedups are possible</a> for
real-world signals that are sparse in the frequency domain.</p>

<h3 id="why-1">Why?</h3>

<p>In a self-tracking context, you might ask the following questions:</p>

<ul>
  <li>Do I have regular exercising patterns?</li>
  <li>Do these patterns cycle weekly? bi-weekly? monthly?</li>
  <li>How much does my amount of exercise fluctuate during a cycle?</li>
</ul>

<p>With the FFT, Fourier analysis can help address these questions. However, it
can only find <em>periodic</em> effects. Unlike linear regression, it does not help
find <em>trends</em> in your data.</p>

<h3 id="the-data-1">The Data</h3>

<p>You can see the code for this analysis <a href="https://github.com/candu/quantified-savagery-files/blob/master/Panic/recovery-journal/food_fft.py">here</a>. Again, I look at the
first 31 days to ensure that the frequency analysis is meaningful.</p>

<p><img src="https://lh5.googleusercontent.com/-8j00ob_Ji-Y/UHXc67MQVpI/AAAAAAAAAC0/n3akVSjtRHs/s640/fft-frequencies.jpg" title="Frequency Strengths" /></p>

<p>There are some apparent maxima there, but it’s hard to tell what they
mean. Part of the difficulty is that <em>these are frequencies rather than
period lengths</em>, so let’s deal with that:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ python food_fft.py 
</span><span class="line">food_fft.py:32: RuntimeWarning: divide by zero encountered in divide
</span><span class="line">  for strength, phase, period in sorted(zip(FS, FP, 1.0 / Q))[-5:]:
</span><span class="line">[2.21 days] 3.0461 (phase=-0.67 days)
</span><span class="line">[-2.21 days] 3.0461 (phase=-0.67 days)
</span><span class="line">[7.75 days] 3.1116 (phase=-3.67 days)
</span><span class="line">[-7.75 days] 3.1116 (phase=-3.67 days)
</span><span class="line">food_fft.py:33: RuntimeWarning: invalid value encountered in double_scalars
</span><span class="line">  phase_days = period * (phase / (2.0 * math.pi))
</span><span class="line">[inf days] 18.1401 (phase=nan days)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>If you’re not familiar with the Fourier transform,
the last line might be a bit mysterious. That corresponds to $ X_0 $, which
is just the sum of the original samples:</p>

<script type="math/tex; mode=display">
X_0 = \sum_{n=0}^{N-1} x_n \cdot e^0 = \sum_{n=0}^{N-1} x_n
</script>

<p>Other than that, the most pronounced cycles have period lengths of
2.21 days and 7.75 days. The former might be explained by a <em>see-saw drinking
pattern</em>, whereas the latter is likely related to the day-of-week effects
we saw <a href="http://candu.github.com/blog/2012/10/08/self-tracking-for-panic-a-deeper-look/">in the previous post</a>.</p>

<p>Which day of the week? The phase is -3.67 days, and our sample starts on a
Monday, placing the first peak on Thursday. The period is slightly longer than
a week, though, and the data runs for 31 days, so these peaks gradually shift
to <em>cover the weekend</em>.</p>

<p>There are two caveats:</p>

<ol>
  <li>I have no idea whether a Fourier coefficient of about 3 is significant
here. If it isn’t, I’m grasping at straws.</li>
  <li>Again, the small amount of data means the frequency domain data is sparse.
To accurately test for bi-daily or weekly effects, I <em>need more
fine-grained period lengths.</em></li>
</ol>

<h2 id="maximum-entropy-modelling">Maximum Entropy Modelling</h2>

<h3 id="what-2">What?</h3>

<p>Maximum entropy modelling answers this question:</p>

<blockquote><p>Given observations of a random process, what is the most likely model<br />for that random process?</p></blockquote>

<p>Given a discrete probability distribution $ p(X = x_k) = p_k $, the entropy
of this distribution is given by</p>

<script type="math/tex; mode=display">
H(p) = \sum - p_k \log p_k
</script>

<p>(Yes, I’m conflating the concepts of
<a href="http://en.wikipedia.org/wiki/Random_variable">random variables</a> and
<a href="http://en.wikipedia.org/wiki/Probability_distribution">probability distributions</a>.
If you knew that, you probably don’t need this explanation.)</p>

<p>This can be thought of as the <em>number of bits needed to encode outcomes
in this distribution</em>. For instance, if I have a double-headed coin, I need
no bits: I already know the outcome. Given a fair coin, though, I need one bit:
heads or tails?</p>

<p>After repeated sampling, we get observed expected values for $ p_k $;
let these be $ p’_k $. Since we would like the model to <em>accurately
reflect what we already know</em>, we impose the constraints $ p_k = p’_k $.
The maximum entropy model is the model that also maximizes $ H(p’) $.</p>

<p>This model encodes what is known
<em>while remaining maximally noncommittal on what is unknown.</em></p>

<p>Adam Berger (CMU) provides <a href="http://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/node2.html#SECTION00011000000000000000">a more concrete example</a>.
If you’re interested in learning more, his tutorial is highly recommended
reading.</p>

<h3 id="why-2">Why?</h3>

<p>In a self-tracking context, you might ask the following questions:</p>

<ul>
  <li>Which treatments have the greatest effect in preventing panic attacks?
Which have the least effect?</li>
  <li>Today I exercised for at least 30 minutes and had four drinks. Am I
likely to get a panic attack?</li>
  <li>What treatments should I try next?</li>
</ul>

<p>Maximum entropy modelling can help address these questions. It is often
used to <em>classify unseen examples</em>, and would be fantastic in a
<a href="http://100plus.com/2012/09/qs-data-commons/">data commons</a> scenario
with enough data to provide recommendations to users. </p>

<h3 id="feature-extraction">Feature Extraction</h3>

<p>Since I’m now effectively building a classifier, there’s an additional step.
I need features for my classifier, which I extract from my existing datasets:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class=""><span class="line">train_set = []
</span><span class="line">dates = set(W).intersection(F)
</span><span class="line">for ds in dates:
</span><span class="line">  try:
</span><span class="line">    ds_data = {
</span><span class="line">      'relaxation' : bool(int(W[ds]['relaxation'])),
</span><span class="line">      'exercise' : bool(int(W[ds]['exercise'])),
</span><span class="line">      'caffeine' : int(F[ds]['caffeine']) &gt; 0,
</span><span class="line">      'sweets' : int(F[ds]['sweets']) &gt; 1,
</span><span class="line">      'alcohol' : int(F[ds]['alcohol']) &gt; 4,
</span><span class="line">      'supplements' : bool(int(F[ds]['supplements']))
</span><span class="line">    }
</span><span class="line">  except (ValueError, KeyError):
</span><span class="line">    continue
</span><span class="line">  had_panic = P.get(ds) and 'panic' or 'no-panic'
</span><span class="line">  train_set.append((ds_data, had_panic))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Note that the features listed here are binary. I use my daily goals as
thresholds on caffeine, sweets, and alcohol.</p>

<p>(If you know how to get float-valued features working with NLTK, let me know!
Otherwise, there’s always <a href="http://www.cs.utah.edu/~hal/megam/">megam</a> or
<a href="http://www-i6.informatik.rwth-aachen.de/web/Software/YASMET.html">YASMET</a>.</p>

<h3 id="the-data-2">The Data</h3>

<p>You can see the code for this analysis <a href="https://github.com/candu/quantified-savagery-files/blob/master/Panic/recovery-journal/panic_maxent.py">here</a>.
This time I don’t care about having consecutive dates, so I use all of the
samples</p>

<p>After building a <code>MaxentClassifier</code>, I print out the most informative features
with <code>show_most_informative_features()</code>:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">-2.204 exercise==True and label is 'panic'
</span><span class="line">   1.821 caffeine==True and label is 'panic'
</span><span class="line">  -0.867 relaxation==True and label is 'panic'
</span><span class="line">   0.741 alcohol==True and label is 'panic'
</span><span class="line">  -0.615 caffeine==True and label is 'no-panic'
</span><span class="line">  -0.537 supplements==True and label is 'panic'
</span><span class="line">   0.439 sweets==True and label is 'panic'
</span><span class="line">   0.430 exercise==True and label is 'no-panic'
</span><span class="line">   0.284 relaxation==True and label is 'no-panic'
</span><span class="line">   0.233 supplements==True and label is 'no-panic'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Exercise, relaxation breathing, and vitamin supplements help with panic.
Caffeine, alcohol, and sweets do not. I knew that already, but this suggests 
<em>which treatments or dietary factors have greatest impact.</em></p>

<p>Let’s consider the supplements finding more closely. Of the 45 days, I took
supplements on all but two. It’s <em>dangerous</em> to draw any conclusions from a
feature for which there are very few negative samples.
This points out some important points about data analysis:</p>

<ul>
  <li><strong>Know your data</strong>: otherwise, you may <em>ascribe undue meaning to outliers or noise.</em></li>
  <li><strong>Know your features:</strong> supplements are probably not a good feature here.
A <em>feature inclusion threshold</em> on number of positive and negative samples 
might be helpful here.</li>
  <li><strong>Beware magic:</strong> even when you understand their inner workings, <em>machine
learning algorithms can produce results that are difficult to interpret.</em></li>
</ul>

<h2 id="up-next">Up Next</h2>

<p>In my next post, I look at a panic recovery dataset gathered using
<a href="https://github.com/candu/qs-counters">qs-counters</a>, a simple utility I built to reduce friction in
self-tracking. I perform these same three analyses on the
<a href="https://github.com/candu/quantified-savagery-files/tree/master/Panic/qs-counters">qs-counters dataset</a>, then compare it to the
<a href="https://github.com/candu/quantified-savagery-files/tree/master/Panic/recovery-journal">recovery-journal dataset</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Self-Tracking For Panic: A bash-ful Look At Some Data]]></title>
    <link href="http://candu.github.com/blog/2012/10/08/self-tracking-for-panic-a-deeper-look/"/>
    <updated>2012-10-08T10:00:00-04:00</updated>
    <id>http://candu.github.com/blog/2012/10/08/self-tracking-for-panic-a-deeper-look</id>
    <content type="html"><![CDATA[<p>In this post, I perform initial exploratory analysis on my panic recovery
journal data using basic UNIX/bash commands.</p>

<!-- more -->

<h2 id="unix-bash-youre-not-serious-right">UNIX? bash? You’re not serious, right?</h2>

<p>Most of the data-centric Quantified Self talks I’ve seen focus on more
complicated methods, including:</p>

<ul>
  <li><a href="http://en.wikipedia.org/wiki/Linear_regression">linear regression</a>, which <em>identifies gradual trends</em>;</li>
  <li><a href="http://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>, which <em>identifies periodic effects</em>;</li>
  <li><a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson’s r</a>, which <em>measures correlation between datasets</em>;</li>
  <li><a href="http://en.wikipedia.org/wiki/Student's_t-test">t-test</a>, which <em>measures difference between datasets</em>.</li>
</ul>

<p>These are extremely powerful tools to have at your disposal. Better yet,
many languages have community-contributed libraries that provide these
tools out-of-the-box. For instance, Python’s <a href="LINK">scipy</a>
offers <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html">linregress</a>
for performing linear regression.</p>

<p>That said, these tools rely on mathematics that is <em>opaque</em> to many software
developers. Even if you don’t need to know how they work to use them, you need
some knowledge of <em>what they do</em> and <em>where they are most appropriate</em>.
Statistical tests in particular often have <em>strong preconditions</em> for use:</p>

<blockquote><p>Each of the two populations being compared should follow a normal distribution.</p><footer><strong>Wikipedia</strong> <cite><a href="http://en.wikipedia.org/wiki/Student%27s_t-test">Student&#8217;s T-test</a></cite></footer></blockquote>

<p>Even if you pick the right tool, there’s still <em>fear associated with losing
control</em>. These tools are not hammers and screwdrivers but magic
wands, and <a href="http://www.flickr.com/photos/wishingline/7162517642/">we are terrible magicians</a>.</p>

<h3 id="a-word-on-exploratory-analysis">A Word On Exploratory Analysis</h3>

<p>I mentioned that this post would demonstrate <em>exploratory analysis</em>. This is
a mode of analysis where you explore your data, play around with it a bit,
grab some low-hanging analytical fruit. You don’t necessarily need higher
mathematics. Regular counts and averages will do. You’re not looking for
ironclad proof, but rather for <em>suggestions</em>.</p>

<blockquote><p>What does this data suggest?</p></blockquote>

<p>This is an important question. Put this way, <em>there is no “right” or “wrong”
way to analyze your data</em>. UNIX tools fit in nicely here, because you can
piece them together and pretty quickly get some useful insights. Better yet,
since you understand what you just did, you can explain it to someone else.
Analysis becomes a <em>demystified</em> and <em>shareable</em> process.</p>

<p>Exploratory analysis is also a <em>great entry point</em> to deeper and more directed
analysis. As you work with the data, you ask more complicated questions. Eventually these
questions exceed the sophistication of your tools, so you look for better
tools. You might not deeply understand the better tools, but at least you’ve
worked with the data a bit. You can <em>perform basic sanity checks</em> when these
better tools turn up results you don’t expect.</p>

<h2 id="the-data">The Data</h2>

<p>I took my paper recovery journal logs:</p>

<p><img src="https://lh6.googleusercontent.com/-TDKFRsDfutE/UHNEPJReCOI/AAAAAAAAABU/q0sWUwRbPoE/s640/IMG_20121005_171146_426.jpg" title="A page from my journal" /></p>

<p>and manually converted them to handy CSV files:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">date,relaxation,exercise,diet,supplements
</span><span class="line">...
</span><span class="line">2012-03-12,0,0,1,1
</span><span class="line">2012-03-13,1,0,1,1
</span><span class="line">2012-03-14,1,0,0,1
</span><span class="line">2012-03-15,1,1,1,1
</span><span class="line">2012-03-16,1,1,1,1
</span><span class="line">2012-03-17,1,1,0,1
</span><span class="line">2012-03-18,0,1,0,1
</span><span class="line">...</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Where did all those different treatments go? I didn’t end up using most of
them. Making nine parallel habit changes is difficult, so I rapidly converged
on a subset of four:</p>

<ul>
  <li>relaxation breathing;</li>
  <li>daily exercise;</li>
  <li>dietary modifications; and</li>
  <li>vitamin supplements.</li>
</ul>

<p>Why manual input? There wasn’t enough data to make
<a href="http://code.google.com/p/tesseract-ocr/">OCR</a>
worthwhile:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ cd recovery-journal
</span><span class="line">$ wc -l * | grep total
</span><span class="line">      41 exercise-record
</span><span class="line">      46 food-diary
</span><span class="line">       8 panic-log
</span><span class="line">      46 weekly-practice-record
</span><span class="line">     141 total</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>You can view and download the raw data files
<a href="https://github.com/candu/quantified-savagery-files/tree/master/Panic/recovery-journal">here</a>.</p>

<h2 id="common-operations">Common Operations</h2>

<p>These operations appear several times in the UNIX one-liners below, so let’s go over
them quickly.</p>

<p>To lop off the CSV column name header:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">tail -n+2</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To extract field <span>$ n $</span> from a CSV file:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">cut -d',' -f$n</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To tabulate counts in descending order:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sort | uniq -c | sort -rn</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To sum a series of numbers:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">awk '{sum+=$1} END {print sum}'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To get the day before <code>$ds</code>:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ts=$(date -j -f "%Y-%m-%d" $ds "+%s"); tsprev=$(echo "$ts - 86400" | bc); dsprev=$(date -j -f "%s" $tsprev "+%Y-%m-%d");</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="and-now-the-main-show">And Now, The Main Show</h2>

<p>Let’s start by looking at my weekly practice record:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ for a in [01] 1; do for b in [01] 1; do for c in [01] 1; do for d in [01] 1; do count=$(grep -E ",$a,$b,$c,$d$" weekly-practice-record | wc -l); echo $a $b $c $d $count; done; done; done; done | tr ' ' '\t'
</span><span class="line">[01]    [01]    [01]    [01]    45
</span><span class="line">[01]    [01]    [01]    1       43
</span><span class="line">[01]    [01]    1       [01]    22
</span><span class="line">[01]    [01]    1       1       21
</span><span class="line">[01]    1       [01]    [01]    32
</span><span class="line">[01]    1       [01]    1       31
</span><span class="line">[01]    1       1       [01]    16
</span><span class="line">[01]    1       1       1       16
</span><span class="line">1       [01]    [01]    [01]    36
</span><span class="line">1       [01]    [01]    1       34
</span><span class="line">1       [01]    1       [01]    19
</span><span class="line">1       [01]    1       1       18
</span><span class="line">1       1       [01]    [01]    26
</span><span class="line">1       1       [01]    1       25
</span><span class="line">1       1       1       [01]    14
</span><span class="line">1       1       1       1       14</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I tracked myself for 45 days. During that time, I followed all four treatments
on 14 days. In order from most to least regular:</p>

<ul>
  <li>vitamin supplements (43 days);</li>
  <li>relaxation breathing (36 days);</li>
  <li>daily exercise (32 days);</li>
  <li>dietary modifications (22 days).</li>
</ul>

<p>I followed both the exercise and diet treatments for only 16 of 45 days! Right away, I
have a question for further inquiry:</p>

<blockquote><p>What was so hard about those two treatments?</p></blockquote>

<h3 id="exercise">Exercise</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ tail -n+2 exercise-record | cut -d',' -f2 | sort | uniq -c | sort -rn | head -5
</span><span class="line">  11 16:00
</span><span class="line">   8 20:00
</span><span class="line">   3 15:00
</span><span class="line">   3 14:00
</span><span class="line">   3 12:00</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>My most common exercise times were 4pm and 8pm. What was I doing at those times?</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ grep 16:00 exercise-record | cut -d',' -f3 | sort | uniq -c | sort -rn | head -1
</span><span class="line">   9 conditioning
</span><span class="line">$ grep 20:00 exercise-record | cut -d',' -f3 | sort | uniq -c | sort -rn | head -1
</span><span class="line">   6 soccer</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Aha! 4pm was my scheduled gym time at work, and 8pm was when I went for
<a href="http://soccerfours.com/">weekly pickup soccer</a>. Both were regularly scheduled activities.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ grep -E "(00|01|02|03|04|05|06|07|08|09|10|11):00" exercise-record | wc -l
</span><span class="line">       7
</span><span class="line">$ grep -E "(12|13|14|15|16|17|18|19|20|21|22|23):00" exercise-record | wc -l
</span><span class="line">       33</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I rarely exercise in the morning, which might be okay: physical performance is
<a href="http://online.wsj.com/article/SB10000872396390444180004578018294057070544.html">higher in the afternoon</a>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ tail -n+2 exercise-record | cut -d',' -f3 | sort | uniq -c | sort -rn
</span><span class="line">  15 conditioning
</span><span class="line">   7 soccer
</span><span class="line">   6 walking
</span><span class="line">   6 cycling
</span><span class="line">   2 running
</span><span class="line">   2 dancing
</span><span class="line">   1 swimming
</span><span class="line">   1 longboarding</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>It’s not surprising to see gym conditioning sets and soccer as my top
activities, but walking and cycling aren’t far behind.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ tail -n+2 exercise-record | cut -d',' -f4 | sort | uniq -c | sort -rn
</span><span class="line">  20 30
</span><span class="line">  11 60
</span><span class="line">   4 45
</span><span class="line">   2 40
</span><span class="line">   2 240
</span><span class="line">   1 120</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I most commonly exercised for 30-60 minutes, with infrequent longer blocks
of activity. What was I doing in those longer blocks?</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ grep -E ",(120|240)$" exercise-record 
</span><span class="line">2012-01-27,20:00,dancing,120
</span><span class="line">2012-01-29,10:00,walking,240
</span><span class="line">2012-02-11,12:00,walking,240</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>When else was I dancing?</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ grep dancing exercise-record
</span><span class="line">2012-01-27,20:00,dancing,120
</span><span class="line">2012-02-03,21:00,dancing,30</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Looking at my calendar, these blocks are easily identified:</p>

<p><img src="https://lh6.googleusercontent.com/-BMXU2Ek3Ng0/UHNJnPz-bqI/AAAAAAAAABw/3vSmmAKQzQo/s800/Screen%2520Shot%25202012-10-08%2520at%25205.45.17%2520PM.jpg" title="Jan 27, 2012" />
<img src="https://lh4.googleusercontent.com/-EWShEmAoYPc/UHNJnuc6eGI/AAAAAAAAAB4/nWSI-zqtp_U/s800/Screen%2520Shot%25202012-10-08%2520at%25205.45.37%2520PM.jpg" title="Feb 03, 2012" /></p>

<p>Having fun is great for my health!</p>

<h3 id="diet">Diet</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ for i in $(seq 2 5); do count=$(cut -d',' -f$i food-diary | awk '{ sum+=$1} END {print sum}'); name=$(head -1 food-diary | cut -d',' -f$i); printf "%12s\t%s\n" $name $count; done
</span><span class="line">    caffeine    6
</span><span class="line">      sweets    48
</span><span class="line">     alcohol    140
</span><span class="line"> supplements    42</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I nearly eliminated caffeine during this period! By the time I started keeping the log,
I’d already started to reduce my consumption. On average, I had just over one sweet per day.
More troubling is alcohol, with an average of 3.1 drinks/day. Let’s take a closer look
at my drinking patterns.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ tail -n+2 food-diary | cut -d',' -f4 | sort | uniq -c | sort -rn
</span><span class="line">  12 4
</span><span class="line">   9 2
</span><span class="line">   7 1
</span><span class="line">   6 5
</span><span class="line">   3 3
</span><span class="line">   2 8
</span><span class="line">   2 6
</span><span class="line">   2 0
</span><span class="line">   2</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>My most common daily drinking amounts were 1, 2, and 4 drinks per day. It was
very rare for me to go a day without drinking any alcohol. More alarmingly,
<a href="http://en.wikipedia.org/wiki/Binge_drinking#Definition">binge drinking</a> counts for <em>over 40% of my alcohol consumption!</em></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ tail -n+2 food-diary | while read line; do weekday=$(date -j -f "%Y-%m-%d" $(echo $line | cut -d',' -f1) "+%a"); alcohol=$(echo $line | cut -d',' -f4); echo $weekday $alcohol; done &gt; drinking.log
</span><span class="line">$ for weekday in Mon Tue Wed Thu Fri Sat Sun; do count=$(grep $weekday drinking.log | cut -d' ' -f2 | awk '{ sum+=$1} END {print sum}'); echo $count $weekday; done | sort -rn
</span><span class="line">28 Wed
</span><span class="line">27 Sat
</span><span class="line">23 Mon
</span><span class="line">20 Sun
</span><span class="line">19 Fri
</span><span class="line">15 Tue
</span><span class="line">8 Thu</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I drank most on Wednesdays and Saturdays; Mondays were also major drinking days,
which is surprising! By contrast, I drank much less than average on Thursdays.
When I narrow in on binge drinking, the pattern shifts slightly:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ grep -E "(5|6|7|8)$" drinking.log | cut -d' ' -f1 | sort | uniq -c | sort -rn
</span><span class="line">   4 Sat
</span><span class="line">   3 Sun
</span><span class="line">   2 Wed
</span><span class="line">   1 Fri</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Wednesday is still an offender, but the weekends are clear culprits. <em>80% of my
binge drinking days fell on weekends.</em></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ tail -n+2 food-diary | cut -d',' -f1,4 | grep -E "(5|6|7|8)$" | while read line; do ds=$(echo $line | cut -d',' -f1); ts=$(date -j -f "%Y-%m-%d" $ds "+%s"); ts_next=$(echo "$ts + 86400" | bc); ds_next=$(date -j -f "%s" $ts_next "+%Y-%m-%d"); echo $line $(grep $ds_next food-diary | cut -d',' -f1,4); done
</span><span class="line">2012-01-21,5 2012-01-22,5
</span><span class="line">2012-01-22,5 2012-01-23,1
</span><span class="line">2012-01-28,8 2012-01-29,2
</span><span class="line">2012-02-01,6 2012-02-02,0
</span><span class="line">2012-02-04,5 2012-02-05,3
</span><span class="line">2012-02-10,6 2012-02-11,4
</span><span class="line">2012-02-12,5 2012-02-13,3
</span><span class="line">2012-03-14,8 2012-03-15,0
</span><span class="line">2012-03-17,5 2012-03-18,5
</span><span class="line">2012-03-18,5 2012-03-19,4</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Among days where I had 5 or more drinks, I had an average of 2.7 drinks the next day.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ tail -n+2 food-diary | cut -d',' -f1,4 | grep -E "(0|1)$" | while read line; do ds=$(echo $line | cut -d',' -f1); ts=$(date -j -f "%Y-%m-%d" $ds "+%s"); tsprev=$(echo "$ts - 86400" | bc); dsprev=$(date -j -f "%s" $tsprev "+%Y-%m-%d"); echo $(grep $dsprev food-diary | cut -d',' -f1,4) $line; done
</span><span class="line">2012-01-22,5 2012-01-23,1
</span><span class="line">2012-01-23,1 2012-01-24,1
</span><span class="line">2012-01-30,4 2012-01-31,1
</span><span class="line">2012-02-01,6 2012-02-02,0
</span><span class="line">2012-02-05,3 2012-02-06,1
</span><span class="line">2012-02-06,1 2012-02-07,1
</span><span class="line">2012-02-08,4 2012-02-09,1
</span><span class="line">2012-03-14,8 2012-03-15,0
</span><span class="line">2012-03-15,0 2012-03-16,1</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Among days where I had fewer than 2 drinks, I had consumed an average of 3.6 drinks the
previous day. This suggests a <em>see-saw pattern</em>: I would drink too much one day,
back off the next, and repeat.</p>

<h3 id="panic">Panic</h3>

<p>All of this skirts the real question:</p>

<blockquote><p>What caused me to have panic attacks?</p></blockquote>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ for i in $(seq 2 4); do head -1 food-diary | cut -d',' -f$i; tail -n+2 panic-log | cut -d',' -f1 | while read ds; do ts=$(date -j -f "%Y-%m-%d" $ds "+%s"); tsprev=$(echo "$ts - 86400" | bc); dsprev=$(date -j -f "%s" $tsprev "+%Y-%m-%d"); echo $(grep $dsprev food-diary | cut -d',' -f1,2) $(grep $ds food-diary | cut -d',' -f1,$i) $ds; done; done
</span><span class="line">caffeine
</span><span class="line">2012-01-28,0 2012-01-29,0 2012-01-29
</span><span class="line">2012-01-31,0 2012-02-01,0 2012-02-01
</span><span class="line">2012-02-03,0 2012-02-04,0 2012-02-04
</span><span class="line">2012-02-07,0 2012-02-08,1 2012-02-08
</span><span class="line">2012-02-12,0 2012-02-13,0 2012-02-13
</span><span class="line">2012-02-29
</span><span class="line">2012-03-12,0 2012-03-13,1 2012-03-13
</span><span class="line">sweets
</span><span class="line">2012-01-28,0 2012-01-29,3 2012-01-29
</span><span class="line">2012-01-31,0 2012-02-01,1 2012-02-01
</span><span class="line">2012-02-03,0 2012-02-04,2 2012-02-04
</span><span class="line">2012-02-07,0 2012-02-08,1 2012-02-08
</span><span class="line">2012-02-12,0 2012-02-13,1 2012-02-13
</span><span class="line">2012-02-29
</span><span class="line">2012-03-12,0 2012-03-13,1 2012-03-13
</span><span class="line">alcohol
</span><span class="line">2012-01-28,0 2012-01-29,2 2012-01-29
</span><span class="line">2012-01-31,0 2012-02-01,6 2012-02-01
</span><span class="line">2012-02-03,0 2012-02-04,5 2012-02-04
</span><span class="line">2012-02-07,0 2012-02-08,4 2012-02-08
</span><span class="line">2012-02-12,0 2012-02-13,3 2012-02-13
</span><span class="line">2012-02-29
</span><span class="line">2012-03-12,0 2012-03-13,2 2012-03-13</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I had no data for <code>2012-02-28</code>. Other than that, on days where I had reported
panic attacks, my <em>current- and previous-day consumption patterns</em> were:</p>

<ul>
  <li><strong>alcohol</strong>: 3.7 drinks that day, 3.8 the previous day (overall average is 3.1);</li>
  <li><strong>sweets</strong>: 1.5 sweets that day, 1.0 the previous day (overall average is 1.0);</li>
  <li><strong>caffeine</strong>: 0.3 caffeinated beverages that day, 0.0 the previous day (overall average is 0.1).</li>
</ul>

<p>This suggests that <em>reducing alcohol and sweets consumption does help</em>. The data
is less clear on caffeine; as previously mentioned, I had mostly cut out
caffeine by the time I started tracking.</p>

<h2 id="up-next">Up Next</h2>

<p>In the next post, I’ll run some of the statistical tests and transformations
mentioned previously on this same data. I’ll also compare this dataset with
another dataset gathered through
<a href="https://github.com/candu/qs-counters">qs-counters</a>, a simple lightweight tracking utility I built to
reduce friction in the recording process.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Panic!]]></title>
    <link href="http://candu.github.com/blog/2012/10/03/panic/"/>
    <updated>2012-10-03T17:47:00-04:00</updated>
    <id>http://candu.github.com/blog/2012/10/03/panic</id>
    <content type="html"><![CDATA[<p>In this post, I’ll tell the story of how I got started with self-tracking
and talk briefly about my first experiment.</p>

<!--more-->

<h2 id="my-journey-into-the-self-tracking-jungle">My Journey Into The Self-Tracking Jungle</h2>

<h3 id="first-a-video-for-context">First, A Video For Context</h3>

<p>If you’ve already seen my talk on panic attacks, feel free to skip to the next
section.</p>

<iframe src="http://player.vimeo.com/video/45860129" width="500" height="281" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
<p><a href="http://vimeo.com/45860129">Evan Savage - Panic</a> from <a href="http://vimeo.com/quantifiedself">Gary Wolf</a> on <a href="http://vimeo.com">Vimeo</a>.</p>

<h3 id="seeking-help">Seeking Help</h3>

<p>My success in confronting panic began with a <em>simple yet powerful insight</em>:</p>

<blockquote><p>I don&#8217;t know how to deal with this, but someone else might.</p></blockquote>

<p>Once I had this insight, <em>seeing a psychologist</em> was the natural next step.</p>

<ul>
  <li>As a <em>domain expert</em>, the psychologist knows where to find relevant
information for a wide variety of conditions. I can ask her for further
resources and receive <em>highly targeted recommendations</em>.</li>
  <li>As a <em>stranger</em>, the psychologist provides an <em>impartial and
non-judgmental sounding board</em>. I can discuss my thoughts, fears, and
experiences with her and feel safe doing so.</li>
</ul>

<p>As a side note, the former point reflects some of the promise of the
<em>Quantified Mass</em>. When you have a specific problem,
there is a subtle but crucial difference between</p>

<blockquote><p>What did others try?</p></blockquote>

<p>and</p>

<blockquote><p>What should I try next?</p></blockquote>

<p>While answering the first question is helpful, I’d argue that answering the
second is an order of magnitude more helpful.</p>

<h3 id="basic-research">Basic Research</h3>

<p>My psychologist recommended the
<a href="http://www.amazon.com/Anxiety-Phobia-Workbook-Edmund-Bourne/dp/1572248912">Anxiety and Phobia Workbook</a>.
As a survey of known symptoms, studies, treatments, and experiences, it gave me
a <em>much broader set of external inputs</em> to draw on. After reading the workbook
cover to cover, the natural next step was now to <em>combine these inputs into
something actionable</em>.</p>

<p>I identified specific treatments that seemed easy to implement. In retrospect,
my initial list was pretty large:</p>

<ul>
  <li><strong>Abdominal Breathing</strong>: by learning to <em>breathe from the abdomen</em>, you train
your body to avoid the sort of shallow chest breathing that can worsen an
attack.</li>
  <li><strong>Deep Relaxation</strong>: through <em>meditation, progressive muscle relaxation, or
other prolonged exercises</em>, you remove sources of physical tension in the body.</li>
  <li><strong>Daily Exercise</strong>: with <em>30 minutes of exercise per day</em>, you reduce overall
stress and increase fitness.</li>
  <li><strong>Positive Self-Talk</strong>: by <em>replacing internal
monologues</em> like “oh no, I’m having a heart attack” with more positive ones
like “I’ve dealt with this before, I’m in control”, you stop this
mental feedback loop from making your attack worse.</li>
  <li><strong>Desensitization</strong>: by <em>gradually exposing yourself to panic triggers in a
safe environment</em>, you sever the mental links that tie those triggers to
panic.</li>
  <li><strong>Assertiveness</strong>: by <em>expressing yourself in a constructive and
assertive manner</em>, you gain a sense of control over your environment
that is also useful in thwarting an attack.</li>
  <li><strong>Diet Modification</strong>: by <em>eliminating or reducing consumption of caffeine,
simple sugars, and alcohol</em>, you reduce baseline stress.</li>
  <li><strong>Supplements</strong>: by <em>taking B-complex and C vitamins</em>, your body gets the
raw materials necessary to regulate stress.</li>
</ul>

<p>All of these take <em>at most 30 minutes per day</em>, and many are <em>passive habits</em>
that rely on <em>small behavior modifications</em>.
<a href="https://www.facebook.com/events/268817716510713/">Small changes in habit</a>
are often more effective than large changes, as they are <em>easier to maintain</em>.</p>

<h2 id="my-first-experiment">My First Experiment</h2>

<p>Building this list led me to another question:</p>

<blockquote><p>How will I know if my condition is improving?</p></blockquote>

<p>This is where <em>self-tracking</em> comes in. To answer this question, I needed
to know what I was doing and whether it was working. I decided that I would
keep a <em>recovery journal</em>, which I divided into four sections.</p>

<ol>
  <li><strong>Weekly Practice Record</strong>: this was an overview of my activity. Every day,
I would check off each treatment I successfully followed. I also had areas
for weekly goals and notes.</li>
  <li><strong>Daily Record of Exercise</strong>: every day, I would fill in either the duration
and type of exercise or a reason for not exercising.</li>
  <li><strong>Food Diary</strong>: every day, I would fill in my caffeine, sugar, and alcohol
consumption. I would also fill whether I took B-complex and C vitamins.</li>
  <li><strong>Panic Triggers and Responses</strong>: if I experienced a panic attack, I would
note the date and time, the severity, what triggered it, what specific
symptoms I experienced, and how I dealt with it.</li>
</ol>

<p>You can view and print the log sheets
<a href="https://docs.google.com/folder/d/0B4lRh7NaNiTMNDE2ODE3ZTMtNWVjZC00M2VlLTg1NWUtZjdmZTlkMGI2NTZm/edit">on Google Docs</a>.</p>

<p>Keeping these logs took no more than five minutes per day. Tracking mechanisms
are most effective when they have <em>low overhead</em>, as this lowers the willpower
barrier to using them regularly.</p>

<h3 id="a-diversion-on-self-tracking-design">A Diversion On Self-Tracking Design</h3>

<blockquote><p>How can we design systems when we don&#8217;t know what we&#8217;re doing?</p><footer><strong>Bret Victor</strong> <cite><a href="http://worrydream.com/LadderOfAbstraction/">Up and Down the Ladder of Abstraction</a></cite></footer></blockquote>

<p>Although I cribbed the individual sections almost verbatim from the workbook,
their specific combination has some curious results.</p>

<p>The <em>broad</em> view of Section 1 is complemented by the <em>deep</em> view of Sections
2-3. In the data visualization world, having
<a href="http://worrydream.com/LadderOfAbstraction/">multiple levels of abstraction</a>
helps the viewer grasp the whole picture without losing their hold on specifics.
By looking at the broad view, I knew my overall progress; by looking at the
deep views, I could see the areas I needed to focus on.</p>

<p>Section 4 provides the <em>feedback loop</em>. Without this section,
I can’t answer my earlier question:</p>

<blockquote><p>How will I know if my condition is improving?</p></blockquote>

<p>My self-tracking was very <em>goal-directed</em>: I had a specific problem that I
wanted to solve. There is another kind of self-tracking, one that I
think many people ignore, and that is <em>exploratory</em> self-tracking.</p>

<p>Imagine this same journal without Section 4. None of the treatments are
specific to panic, so they could reasonably be followed by anyone. Without the
goal of confronting panic, there is <em>greater room for curiosity</em>. You could add
more experiments, play with correlations, ask weirder questions like
“what happens if I eat a lot of <a href="http://quantifiedself.com/butter/">butter</a>?”
<em>Without imposing goals, there is no failure or success</em>, and that can be both
a curse and a blessing. The curse is that you might not measurably improve
yourself. The blessing is that you might not care!</p>

<p>I believe that, much like a
<a href="http://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>
system, the Quantified Self community <em>needs both modes of self-tracking to
thrive</em>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Welcome to Quantified Savagery]]></title>
    <link href="http://candu.github.com/blog/2012/10/02/welcome-to-quantified-savagery/"/>
    <updated>2012-10-02T21:34:00-04:00</updated>
    <id>http://candu.github.com/blog/2012/10/02/welcome-to-quantified-savagery</id>
    <content type="html"><![CDATA[<p>I’m <a href="http://cv.savageevan.com/">Evan Savage</a> and I’d like to welcome you
to Quantified Savagery. I recently left my job at Facebook to focus on
exploring the
<a href="http://quantifiedself.com/">Quantified Self</a>,
and I’m super-excited to share those explorations here on my new blog.</p>

<p>In this first post, I’ll explain the Quantified Self, give you a sense of
what I’ll be posting in the near future, and provide some helpful tips on
reading this blog.</p>

<!--more-->

<h2 id="what-is-the-quantified-self">What is the Quantified Self?</h2>

<p>I’ve had to explain this countless times to friends, family, and
co-workers: why did I leave
<a href="http://www.glassdoor.com/Reviews/Facebook-Reviews-E40772.htm">one of the world’s best employers</a>
to explore a field most people haven’t even heard of?
I usually start by name-dropping
<a href="http://www.fitbit.com/">Fitbit</a> or <a href="http://nikeplus.nike.com/plus/">Nike+</a> as
prominent examples of personal data collection and analysis. I then do some
semi-coherent hand-waving about the vast potential of data collection and
analysis. All of this is really just an attempt to cover up the fact
that <em>I don’t really know</em>.</p>

<p>That doesn’t really cut it as an explanation for a major life decision,
though, so let’s look a bit deeper.</p>

<p>The Quantified Self
<a href="http://quantifiedself.com/">community website</a> has this tagline:</p>

<blockquote><p>self knowledge through numbers</p></blockquote>

<ul>
  <li><strong>self:</strong> you seek to answer <em>your</em> questions.</li>
  <li><strong>knowledge:</strong> in doing this, you gain an <em>awareness</em> of your behaviors and
motivations.</li>
  <li><strong>through numbers:</strong> this process is driven by <em>data</em> gathered from sensors,
journals, and any other tools at your disposal.</li>
</ul>

<p>This is a good first-level approximation: you <em>gather</em> your data, <em>analyze</em> it,
and <em>interpret</em> the analysis to become more <em>self-aware</em>.</p>

<p><span class="pullquote-right" data-pullquote="For the first time in history, over half the world&#8217;s population owns sensor-packed networked computing devices.">
But why is this suddenly important? After all, journal-keeping has been around
roughly as long as written languages. The answer lies in <em>technology</em>.
For the first time in history, over half the world’s population owns
sensor-packed networked computing devices. We refer to these
devices as mobile phones only by historical accident. In fact, they’re really
powerful tools for speeding up this process of gaining
<em>self knowledge through numbers</em>.
</span></p>

<p>As I said before, though, this is only a first-level approximation. There are
two main ways in which Quantified Self can achieve greater awesomeness:
<em>Quantified Mass</em> and <em>Qualified Self</em>.</p>

<h3 id="quantified-mass">Quantified Mass</h3>

<p>As Gary Wolf pointed out in his
<a href="http://www.onthemedia.org/people/gary-wolf/">interview with On the Media</a>,
self-tracking doesn’t lead to self-obsession but rather to group-awareness.
In asking our own questions, we find that these questions are important to
others as well. When many people gather comparable datasets to answer the
same questions, there’s an opportunity to extract insights
that could benefit us all.</p>

<p>Scalable <em>mass</em> insights have massive power. Taking a 1% chunk out of the
American obesity epidemic might not sound impressive, but that’s potentially
a <a href="http://www.forbes.com/sites/bethhoffman/2012/08/16/what-the-obesity-epidemic-costs-us-infographic/">$6 billion impact</a>
on direct and indirect costs. And that’s just in the United States, which
counts for a tiny slice of the global mobile userbase.</p>

<p>Many of the requisite data mining tools already
exist, but they’re being employed to increase advertising click-through rates
by 1%. The engineers building these tools aren’t indifferent to societal
problems; rather, <em>the datasets to solve those problems largely don’t exist yet.</em>
Once they do, the quantified mass can start driving these massive-scale
incremental wins.</p>

<h3 id="qualified-self">Qualified Self</h3>

<blockquote><p>Not everything that counts can be counted,<br />and not everything that can be counted counts.</p></blockquote>

<p>Our perception of life is rarely numerical. Much more often, it is visual,
auditory, tactile, or experiential. 
The problem with data is that you can’t
<a href="http://worrydream.com/#!/KillMath">see</a> or
<a href="http://blog.makezine.com/2009/02/08/haptic-compass/">feel</a> them.
Even the best data scientists use
data only as a means of
<a href="http://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen.html">telling a story</a>.</p>

<p>Put another way, this awareness process starts with qualitative questions
and ends with qualitative answers. Data is the intermediate representation,
one we use for its unique ability to permit detailed analysis. Ultimately,
though, we’re going to ask questions like</p>

<blockquote><p>How can I improve my fitness?</p></blockquote>

<p>and expect answers like</p>

<blockquote><p>By finding training partners. By doing more engaging athletic<br />activities. By setting aside regularly scheduled time.</p></blockquote>

<p>We’re going to need moral support. We’re going to give and receive advice.
We’re going to have conversations and <em>tell stories</em> about our personal
struggles with fitness.</p>

<p>These qualified aspects of self-awareness are arguably the most important
to us. <em>Data provide a stepping-stone</em>, something we can build upon
to address these aspects. By building systems designed for the qualified self,
we can bring the benefits of the quantified self to everyone.</p>

<h3 id="back-to-the-quantified-self">Back to the Quantified Self</h3>

<p>That sums up why I’m so excited about Quantified Self: there really is an
enormous potential here to revolutionize our lives on both the global/societal
and individual levels.
It’s also a fantastically diverse field, one that connects hackers and 
doctors and entrepreneurs and teachers and artists through mutual pursuit of
insanely lofty goals.</p>

<h2 id="upcoming-content">Upcoming Content</h2>

<p>The next few posts will detail my experiences <em>dealing with panic
disorder through self-tracking</em>. I gave a talk about this to the Bay Area
Quantified Self community, which you can
<a href="http://quantifiedself.com/2012/07/evan-savage-on-panic-tracking/">view here</a>
for some initial context.</p>

<h2 id="how-to-read-this-blog">How To Read This Blog</h2>

<p>Although this is a blog about personal data, one of my primary goals is to
make the thoughts and insights shared here accessible to a broad audience.
You can filter what you read with these categories:</p>

<ul>
  <li><a href="http://candu.github.com/blog/categories/non-technical/" class="category">Non-Technical</a>:
expect to see insights, thoughts, discussions, descriptions
of planned experiments, and post-mortems. I might link to articles, studies,
or books that provide context, but I’ll try to summarize the relevant
parts.</li>
  <li><a href="http://candu.github.com/blog/categories/technical/" class="category">Technical</a>:
expect to see code, statistical analysis, $ \LaTeX $ formulae,
links to <a href="https://github.com/candu">Github</a> repos, and algorithm
descriptions. I’ll assume familiarity with programming and mathematics, or
at least a willingness to learn.</li>
</ul>

<p>In addition, many of my posts will be connected to one or more experiments.
For instance, my upcoming posts on self-tracking to address panic disorder will
fall under the <a href="http://candu.github.com/blog/categories/panic/" class="category">Panic</a> category.
For every experiment, I’ll attempt to post
content in both the
<a href="http://candu.github.com/blog/categories/non-technical/" class="category">Non-Technical</a> and
<a href="http://candu.github.com/blog/categories/technical/" class="category">Technical</a> categories.</p>

<p>Of course, I’ll be glad to answer any questions you have, technical or
otherwise!</p>
]]></content>
  </entry>
  
</feed>
