<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Non-Technical | Quantified Savagery]]></title>
  <link href="http://blog.savageevan.com/blog/categories/non-technical/atom.xml" rel="self"/>
  <link href="http://blog.savageevan.com/"/>
  <updated>2014-06-20T14:27:44-07:00</updated>
  <id>http://blog.savageevan.com/</id>
  <author>
    <name><![CDATA[Evan Savage]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Tale of Two Trips]]></title>
    <link href="http://blog.savageevan.com/blog/2013/10/09/a-tale-of-two-trips/"/>
    <updated>2013-10-09T09:32:00-07:00</updated>
    <id>http://blog.savageevan.com/blog/2013/10/09/a-tale-of-two-trips</id>
    <content type="html"><![CDATA[<p>This summer, <a href="http://www.eecs.berkeley.edu/~valkyrie/">Valkyrie</a> and I travelled
through Eastern Asia.  We started with the Great Barrier Reef and rainforests near
Cairns, then went onwards to Singapore, Vietnam, Cambodia, Malaysia, and Korea
before spending three typhoon-stricken days holed up in Manila, Philippines.</p>

<p>As with our <a href="http://fearlesstost.github.io/biketotheearth/">bike trip</a>, we kept
a <a href="http://ramblelust.savageinter.net/">daily journal</a> of our travels in blog
form.  In this post, I visualize the two trips from our blog entries using some
simple word count graphs.  These aren’t the most sophisticated visualizations
possible, but they provide a simple starting point to build upon in future
blog posts.</p>

<!-- more -->

<p>First of all, it feels good to write another post!  The rapidly upcoming <a href="http://quantifiedself.com/conference/San-Francisco-2013/">Quantified Self 2013 Global Conference</a>
seemed like a good excuse to break seven months of blog neglect, so here I am.</p>

<p>I’ll refer to these trips as <em>Bike to the Earth</em> and <em>Ramblelust</em> respectively,
following the names of each trip blog.  Let’s start with a look at the broadest
of broad textual measures, word count.</p>

<h2 id="word-count">Word Count</h2>

<p>How many words did we write for each trip?  How many per post, on average?</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ python global_word_count.py &lt; BikeToTheEarth/posts_normalized.json 
</span><span class='line'>141329 words in 197 posts (717 words/post)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>$ python global_word_count.py &lt; Ramblelust/posts_normalized.json 
</span><span class='line'>61010 words in 86 posts (709 words/post)</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Not much difference here.  What happens if we graph the word counts over time?
Top is Bike to the Earth, bottom is Ramblelust.</p>

<p><img src="https://lh4.googleusercontent.com/-5Sv5_JZEDdw/UlcoazZGSxI/AAAAAAAAAks/9TZH7nDzpnQ/w1000/biketotheearth_daily_words.jpg">
<img src="https://lh6.googleusercontent.com/-BW5K-iWb56c/UlcoZ4fktZI/AAAAAAAAAkw/3g2w4bbbMtI/w1000/ramblelust_daily_words.jpg"></p>

<p>One point jumps out on the Bike to the Earth graph: <a href="http://fearlesstost.github.io/biketotheearth/posts/2010/10/19/hellbania.html">this post</a>,
a massive 3600-word whopper that details our arduous exit from the rain-stricken
Albanian coastline and into Greece.  For comparison, our longest post from
Ramblelust was <a href="http://ramblelust.savageinter.net/blog/2013/06/29/cam-ranh-ing-around/">this one</a>
about our CouchSurfing experiences in Cam Ranh, a short distance south from
Nha Trang along the Vietnamese coast.</p>

<p>Still, it’s hard to make much out of such noisy data.  Let’s smooth that a bit
by looking at weekly averages. Again, top is Bike to the Earth, bottom is Ramblelust.</p>

<p><img src="https://lh6.googleusercontent.com/-9YT2XueNpAs/UlcodH_CpwI/AAAAAAAAAkk/0ZnCYmXSMJc/w1000/biketotheearth_daily_words_smoothed.jpg">
<img src="https://lh4.googleusercontent.com/-C-Q604GWq84/UlcoesNibrI/AAAAAAAAAjk/8RHMmaBdOi0/w1000/ramblelust_daily_words_smoothed.jpg"></p>

<p>Now another pattern pops out of the Bike to the Earth data: sometime around
the beginning of September, our writing output doubled.  At that time, we were
just entering Italy, where we were sandwiched in between the southern Alps
and the sea for about 900km before reaching Rome.</p>

<p>As I go through this process, I’m constantly jumping back and forth between
the blog posts and these graphs.  As with all visualization, this context is an
essential part of understanding the data.</p>

<h2 id="next-steps">Next Steps</h2>

<p>There are lots of places we could go from here:</p>

<ul>
  <li>sentiment analysis;</li>
  <li>location-based visualization (by combining with country data);</li>
  <li>authorship analysis (my posts vs. Valkyrie’s posts)…</li>
</ul>

<p>…the list goes on.  What would you like to see?  Comment via Facebook or
Twitter to let me know, and I’ll put it into the next blog post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Another Quadtree Map Rendering]]></title>
    <link href="http://blog.savageevan.com/blog/2013/03/04/another-quadtree-map-rendering/"/>
    <updated>2013-03-04T16:48:00-08:00</updated>
    <id>http://blog.savageevan.com/blog/2013/03/04/another-quadtree-map-rendering</id>
    <content type="html"><![CDATA[<p>This will be a quick post: I’ve got another population-based map rendering to
share, based on the work described in <a href="http://blog.savageevan.com/blog/2013/02/21/quadtree-cartography/">this post</a>.</p>

<!-- more -->

<h2 id="the-rendering">The Rendering</h2>

<p>This rendering uses tiles at Google Maps zoom level 14:</p>

<p><img src="https://lh5.googleusercontent.com/-74zVhVDHIdc/UTVAtI4bhcI/AAAAAAAAAWY/sWT9JplWl7k/s800/tiles14.2048.jpg"></p>

<p>I decided to experiment with solid shading rather than wireframe for the tiles.
This cuts down on the <a href="http://en.wikipedia.org/wiki/Moir%C3%A9_pattern">Moiré effect</a>
in densely populated areas.</p>

<p>The original is a <em>whopping 1 gigapixels</em>, so I had to resize it using
<a href="http://www.imagemagick.org/script/index.php">ImageMagick</a> before uploading it to Picasa:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ convert tiles14.jpg -sample 2048x2048 tiles14.2048.jpg</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>A few more random tidbits of information:</p>

<ul>
  <li>Computing the tile subdivision <em>took one CPU-hour</em> on my laptop, a fairly
new MacBook Air.</li>
  <li>At zoom level 14, tiles near the equator are <em>roughly 1.5 miles to a side.</em>
(Tiles further north or south are shorter in the north-south direction
due to distortion in the Mercator projection.)</li>
  <li><em>The Nile is clearly visible</em> between the Nile Delta and Aswan.</li>
</ul>

<h2 id="next-post">Next Post</h2>

<p>In my next post, I’ll dive into <a href="http://fearlesstost.github.com/biketotheearth/">six months of journal entries</a>
from a <a href="http://goo.gl/maps/0Xs55">six-month bike trip</a> that
<a href="http://www.eecs.berkeley.edu/~valkyrie/">Valkyrie Savage</a> and I took back in
2010.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Quadtree Cartography]]></title>
    <link href="http://blog.savageevan.com/blog/2013/02/21/quadtree-cartography/"/>
    <updated>2013-02-21T10:05:00-08:00</updated>
    <id>http://blog.savageevan.com/blog/2013/02/21/quadtree-cartography</id>
    <content type="html"><![CDATA[<p>In this post, I show off some images from a project I’m working on (which will
remain nameless for now!) These images visualize subdivisions of the Earth
into Google Maps tile-sized regions with roughly equal population. I’ll also
provide a brief and non-technical rundown of the process by which I generated
these images.</p>

<!-- more -->

<h2 id="the-images">The Images</h2>

<p>First, a representative image from my renderings:</p>

<p><img src="https://lh5.googleusercontent.com/-EmN0ma8uhtg/USZnFkGsrfI/AAAAAAAAAV8/hDpa5sx1-IE/s640/tiles11.ag.jpg"></p>

<p>I love working on problems with a visual aspect - you get direct sensory
feedback on your progress!</p>

<p>Here, we can clearly see the continents delineated
by dense coastal population clusters. India and China are especially detailed,
and Europe has fairly uniform density throughout. Contrast this with North
America: northern Canada is sparsely populated, as are the deserts and
mountains of the Central United States.</p>

<p>Next, some renderings at different subdivision levels (Google Maps zoom
levels 8-11):</p>

<p><img src="https://lh4.googleusercontent.com/-SqLGYcz6yl0/USZnBXDoUNI/AAAAAAAAAUs/C7vdLkL522Y/s288/tiles8.ag60.jpg">
<img src="https://lh5.googleusercontent.com/-m2wk-ov9Mw4/USZnBmDUucI/AAAAAAAAAU0/vrU48gzJei4/s288/tiles9.ag60.jpg">
<img src="https://lh6.googleusercontent.com/-GuXnAIjWUf0/USZnCbAac_I/AAAAAAAAAU8/urh1UXBD3-U/s288/tiles10.ag60.jpg">
<img src="https://lh6.googleusercontent.com/-Y53pPe7T0II/USZnCn5ZL3I/AAAAAAAAAVI/Owjf_pnQBic/s288/tiles11.ag60.jpg"></p>

<p>As the subdivision level increases, the continents progress from blocky pixel
art to more recognizable shapes.</p>

<p>Finally, some renderings with different resolutions of the underlying population
data (degree, half-degree, quarter-degree, and 2.5 arc minutes):</p>

<p><img src="https://lh6.googleusercontent.com/-GuXnAIjWUf0/USZnCbAac_I/AAAAAAAAAU8/urh1UXBD3-U/s288/tiles10.ag60.jpg">
<img src="https://lh6.googleusercontent.com/-RRw5Gx4OiaA/USZnEQhqNhI/AAAAAAAAAVg/i6B3yx8BF_M/s288/tiles10.ag30.jpg">
<img src="https://lh5.googleusercontent.com/-NkGpf7zzHsY/USZnFI4qQ7I/AAAAAAAAAV4/Kl1UB4U-4pU/s288/tiles10.ag15.jpg">
<img src="https://lh6.googleusercontent.com/-n6Ufe_6mJzM/USZnF60IfvI/AAAAAAAAAWE/PEmAgvWvzNU/s288/tiles10.ag.jpg"></p>

<p>Here the effect is more subtle: detail is added in densely populated areas,
but larger tiles (corresponding to more remote regions) are mostly unaffected.</p>

<p>To see all the images as small multiples, <a href="https://picasaweb.google.com/100933554722754572774/20130221QuadtreeCartography#">view the album on Picasa</a>.</p>

<h2 id="the-process">The Process</h2>

<p>There are four major steps: getting the data, combining it with Google Maps
tile data, building the subdivision, and rendering it.</p>

<h3 id="getting-the-data">Getting the Data</h3>

<p>The <a href="http://sedac.ciesin.columbia.edu/citations">NASA Socio-Economic Data and Applications Center</a>,
or SEDAC, compiles global population grids. These grids contain the estimated
number of people living in each 2.5-arc-minute square of the Earth’s surface.
2.5 arc-minutes is 1/24 of a degree, or about 4.5 km of equatorial circumference:
definitely high-resolution enough for building some awesome maps!</p>

<p>The population count grids are available <a href="http://sedac.ciesin.columbia.edu/data/set/gpw-v3-population-count/data-download">here</a>.
You have to register on the site and cite usage of their data, but otherwise it
appears to be freely available.</p>

<h3 id="combining-with-google-maps">Combining with Google Maps</h3>

<p>Google Maps uses a <a href="http://en.wikipedia.org/wiki/Mercator_projection">Mercator projection</a>.
This projection is <a href="https://developers.google.com/maps/documentation/javascript/maptypes#WorldCoordinates">truncated</a>
at roughly 85 degrees latitude to create a square map, which is then projected
onto a 256 x 256 world coordinate system. Finally, world coordinates are
mapped to <a href="https://developers.google.com/maps/documentation/javascript/maptypes#PixelCoordinates">pixel coordinates</a>
at different zoom levels, which determine which <a href="https://developers.google.com/maps/documentation/javascript/maptypes#TileCoordinates">tile</a>
your location falls in.</p>

<p>To match up the gridded population data with Google Maps tiles, then, we need
to do the following:</p>

<ul>
  <li>for each grid cell, <em>determine its latitude and longitude boundaries</em>;</li>
  <li>use those boundaries to <em>figure out which map tiles the cell overlaps</em>;</li>
  <li><em>divide the cell’s population among those map tiles</em>.</li>
</ul>

<p>To divide the cell’s population fairly, I determine how much of the cell
overlaps each tile.</p>

<p>I found <a href="https://google-developers.appspot.com/maps/documentation/javascript/examples/map-coordinates">this helpful example</a>
of working with locations, world coordinates, pixel coordinates, and tiles.
The source code of that example contains an implementation of Google’s
Mercator projection, which I built into a larger <a href="http://nodejs.org/">node.js</a>
utility for computing the equal-population subdivision.</p>

<p>(Yes, node.js is fine for CPU-intensive tasks, just not in the same process
as your webserver.)</p>

<h3 id="building-an-equal-population-subdivision">Building an Equal-Population Subdivision</h3>

<p>To get map tiles of equal population, <em>I combine tiles into larger tiles
until the population exceeds a threshold.</em> This creates large tiles in
sparsely populated areas while leaving smaller tiles in densely populated
areas.</p>

<p>(I mentioned <a href="http://blog.notdot.net/2009/11/Damn-Cool-Algorithms-Spatial-indexing-with-Quadtrees-and-Hilbert-Curves">quadtrees</a>
in the title of this post - this data structure is ideally suited for the
problem.)</p>

<h3 id="rendering-the-subdivision">Rendering the Subdivision</h3>

<p>This is the easy part! I used <a href="https://pypi.python.org/pypi/Pillow/">Pillow</a>, a
nicely-packaged version of the excellent <a href="http://www.pythonware.com/products/pil/">Python Imaging Library</a>,
to render the subdivisions out as JPEG images.</p>

<p>(I suppose I could have rendered SVG images in node.js using some <a href="https://github.com/tmpvar/jsdom">jsdom</a>
and <a href="http://d3js.org/">d3</a> hackery, but I was already familiar with using
Python Imaging Library for image synthesis.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What I'll Look Like In 50 Years]]></title>
    <link href="http://blog.savageevan.com/blog/2013/01/31/what-ill-look-like-in-50-years/"/>
    <updated>2013-01-31T12:43:00-08:00</updated>
    <id>http://blog.savageevan.com/blog/2013/01/31/what-ill-look-like-in-50-years</id>
    <content type="html"><![CDATA[<p>I spent a few weeks in the not-so-frozen Canadian northlands over the winter
holidays. While there, I had the chance to visit an old childhood favorite:
the <a href="http://ontariosciencecentre.ca/">Ontario Science Centre</a>, six floors of science-based awesomeness.
One of their current exhibits, the <a href="http://ontariosciencecentre.ca/aging/">Amazing Aging Machine</a>, uses a
computer vision software package called <a href="http://aprilage.com/">APRIL</a> to predict how your
face will change over the next 50 years.</p>

<p>In this post, I explore my results from that exhibit alongside a customized
aging I performed using the <a href="http://www.aprilage.com/AprilAPI_V2.pdf">APRIL API</a>.</p>

<!-- more -->

<h2 id="present-me">Present Me</h2>

<p>It’s not the most flattering photo, but here I am at 26:</p>

<p><img src="https://lh4.googleusercontent.com/-8qaAivSLFrI/UQrbKiU3JZI/AAAAAAAAARY/9e7IvKZUB00/s288/aging1.jpg"></p>

<h2 id="future-me">Future Me</h2>

<h3 id="take-one-amazing-aging-machine">Take One: Amazing Aging Machine</h3>

<p>First, my face balloons out massively:</p>

<p><img src="https://lh4.googleusercontent.com/-LCFkNNtDiJg/UQrbKwGOIQI/AAAAAAAAARg/SS9o98axBtU/s288/aging2.jpg"></p>

<p>Next, my cheek bones set downwards:</p>

<p><img src="https://lh5.googleusercontent.com/-VZhuI1uOVQk/UQrbLTKdBXI/AAAAAAAAARo/LHKV6cGEBGA/s288/aging3.jpg">
<img src="https://lh6.googleusercontent.com/-aNcc-FC_4yk/UQrbL53IPpI/AAAAAAAAARs/DRf3ySCxPs4/s288/aging4.jpg"></p>

<p>Finally, my face leans up and wrinkles a tiny bit:</p>

<p><img src="https://lh6.googleusercontent.com/-F8xBzpDWsM4/UQrbMUOukwI/AAAAAAAAAR0/cRqPDd_wYPM/s288/aging5.jpg"></p>

<h3 id="take-two-april-api">Take Two: APRIL API</h3>

<p>For this run, I had access to the raw aging metadata, so I could see
exactly how old APRIL thought I was at each point in the aging sequence.</p>

<p>From 26 to 28, there’s not much change:</p>

<p><img src="https://lh6.googleusercontent.com/-u36fDGLeI0Y/UQrbNrKtZyI/AAAAAAAAASE/NQSn0Y5uCng/s288/age28.jpg"></p>

<p>Then, by age 35, my face elongates slightly:</p>

<p><img src="https://lh4.googleusercontent.com/-cRxKskiAyos/UQrbOiA_OjI/AAAAAAAAASM/GBFlRfZtXnc/s288/age35.jpg"></p>

<p>I while away the next couple of decades in relative facial stasis. The
most pronounced change is in my skin, which pales gradually with age:</p>

<p><img src="https://lh6.googleusercontent.com/-gYCoSmKbBDw/UQrbO46gohI/AAAAAAAAASY/X8RoWKpGT_8/s288/age47.jpg">
<img src="https://lh6.googleusercontent.com/-dWkaa-neumY/UQrbPgkBVtI/AAAAAAAAASg/ebWh4i14qYk/s288/age55.jpg"></p>

<p>Finally, age catches up with me, and I wrinkle into a haunted
septuagenarian:</p>

<p><img src="https://lh5.googleusercontent.com/-JNBO6QMu-Xg/UQrbP9uzUUI/AAAAAAAAASo/Dj43_1l46Zo/s288/age61.jpg">
<img src="https://lh4.googleusercontent.com/-w7zx8Ql3PnM/UQrbQO2pqVI/AAAAAAAAASw/9HJrW1EUz2c/s288/age67.jpg">
<img src="https://lh5.googleusercontent.com/-RNgo0CglAjs/UQrbQtlY69I/AAAAAAAAAS0/OXD8Yqf63og/s288/age72.jpg"></p>

<p>A few changes, each very minor, contribute to my forlorn expression over
these last three photos.</p>

<ul>
  <li>The <em>eyes get slightly rounder</em>, as though they’re welling up.</li>
  <li><em>Wrinkling above the eyes</em> gives the impression of a furrowed brow.</li>
  <li>The face <em>elongates yet again</em>, creating a drawn expression.</li>
  <li>As part of the elongation of the face, the <em>mouth corners sag downwards</em>
into the merest hint of a frown.</li>
</ul>

<p>Note the lack of deep forehead and upper nose creases which normally
accompany the furrowed brow expression. The mere suggestion of it on the eyes
is enough to trigger our expression recognition! It’s amazing how sensitive
we are to minute variations in facial muscle position.</p>

<h3 id="summary">Summary</h3>

<p>These images provide two divergent visions for my distant future:</p>

<p><img src="https://lh6.googleusercontent.com/-F8xBzpDWsM4/UQrbMUOukwI/AAAAAAAAAR0/cRqPDd_wYPM/s288/aging5.jpg">
<img src="https://lh5.googleusercontent.com/-RNgo0CglAjs/UQrbQtlY69I/AAAAAAAAAS0/OXD8Yqf63og/s288/age72.jpg"></p>

<p>For comparison, here’s my father in his late 50s, looking quite a bit happier:</p>

<p><img src="http://farm4.staticflickr.com/3177/2828216200_5846e31c4a_z.jpg"></p>

<h2 id="why-were-those-so-different">Why Were Those So Different?</h2>

<p><blockquote><p>…the machine uses state-of-the-art aging software developed in partnership with Aprilage Development Inc. of Toronto to add decades to the faces of 8-12 year olds.</p><footer><strong>The Amazing Aging Machine</strong> <cite><a href='http://ontariosciencecentre.ca/aging/'>ontariosciencecentre.ca/aging/&hellip;</a></cite></footer></blockquote></p>

<p>The Amazing Aging Machine is calibrated for ages 8-12, likely to match 
the Ontario Science Centre’s target demographic. (Sadly, I couldn’t find
detailed visitor demographic data!) In my case, this creates an awkward
puffy look: it’s applying changes in facial structure through adolescence,
when much of our bone growth occurs.</p>

<p>By contrast, the APRIL API asks for your current age, allowing it to more
correctly calibrate its models. As a result, the second set of faces exhibits
relatively little change in shape.</p>

<h2 id="what-do-i-get-out-of-this">What Do I Get Out Of This?</h2>

<p>Although my face is unlikely to match either of these faces at 72, this
experiment provides some insight into how our faces change with age. After
all, the APRIL face aging models are based on real face data. They represent
a sort of statistical average of the aging process.</p>

<p>Also, I get the vaguely warm feeling that comes with having contributed to our
<a href="http://vimeo.com/29052688">collective intelligence</a>. I provided APRIL
with a real age-labelled face, which will likely be used to help train future
models.</p>

<h2 id="appendix-how-to-use-the-april-api">Appendix: How To Use The APRIL API</h2>

<p>For the more technically-minded, I’ve provided a quick walkthrough of the
API aging pipeline. For all the gritty details, consult the <a href="http://www.aprilage.com/AprilAPI_V2.pdf">API docs</a>.</p>

<p>Before starting, I highly recommend installing a tool like <a href="https://github.com/jmhodges/jsonpp">jsonpp</a>;
it makes it much easier to read API results.</p>

<p>The first step is manual: you need to register at <a href="http://www.ageme.com/">ageme.com</a>, then
click the confirmation link in your email.</p>

<p><img src="https://lh5.googleusercontent.com/-wuu_sRDb7qQ/UQrtRmoofNI/AAAAAAAAATE/l7CAexc1_ZY/s400/ageme_register.jpg"></p>

<p>The next step is uploading an image, but let’s check first that the API
works by retrieving our user info:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/userInfo
</span><span class='line'>{“result_code”:0,”message”:”Unauthorized”}</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Oops! We haven’t authenticated ourselves. The Authorization header uses a
brain-dead and highly insecure <code>base64</code> encoding:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ python -c “import base64; print base64.encodestring(‘username:password’)”
</span><span class='line'>dXNlcm5hbWU6cGFzc3dvcmQ=</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>(Obviously this isn’t my real username/password. Substitute yours above and
use the resulting <code>base64</code>-encoded string in the <code>Authorization</code> headers
below. I’ll use this bogus value to illustrate the flow.)</p>

<p>With the correct header, we can try fetching the user info again:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/userInfo | jsonpp 
</span><span class='line'>{
</span><span class='line'>  “uri”: “http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com”,
</span><span class='line'>  “email”: “savage.evan@gmail.com”,
</span><span class='line'>  “tokens”: 0,
</span><span class='line'>  “numOfAgings”: 1,
</span><span class='line'>  “role”: “user”
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Great! Now we can POST an image to the uploading endpoint with <code>curl</code>:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -F ‘filename=aging1.jpg’ -F ‘image=@/Users/candu/Desktop/aging1.jpg’ -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/images</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Another manual step: before proceeding, you’ll need to purchase a token on
the <a href="http://www.ageme.com/">ageme.com</a> site. At time of writing, this cost $3.99; I looked for
active promotion codes, but couldn’t find any.</p>

<p><img src="https://lh6.googleusercontent.com/-co9vyFu4uJQ/UQrtSM1T8mI/AAAAAAAAATM/8ozrK3aiGjk/s400/ageme_buytokens.jpg"></p>

<p>With your aging token purchased, you can now create an aging document. This
lets APRIL know your age and ethnicity, which helps it to select the
appropriate models for your particular aging sequence. It also identifies the
starting image of that sequence via the <code>imageId</code> returned during image upload.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -H ‘Content-Type: application/json’ -d ‘{“document”: {“gender”: “male”, “age”: 26, “name”: “Evan”, “ethnicity”: “Caucasian”}, “imageId”: 2371944}}’ -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/documents</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>We’re ready to run the aging process. There’s a single method <code>detectMatchAge</code>
for performing all three steps, but I’ll break it down into the component
steps here:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -X POST -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/documents/2371973/pointDetection
</span><span class='line'>$ while true; do curl -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/documents/2371973/status; done
</span><span class='line'>$ curl -X POST -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/documents/2371973/match
</span><span class='line'>$ while true; do curl -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/documents/2371973/status; done
</span><span class='line'>$ curl -H ‘Content-Type: application/json’ -d ‘{“sequenceType”: “Max72”, “sequences”: [{“smoking”: 0, “sunExposure”: 0, “multiplier”: 1}]}’ -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/documents/2371973/aging
</span><span class='line'>$ while true; do curl -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/documents/2371973/status; done</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Note the <code>while</code> loops, which wait for each step to complete. Once all steps
are completed, we retrieve the aging results:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -H “Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=” http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/documents/2371973/results &gt; aging_results.json
</span><span class='line'>$ cat aging_results.json | jsonpp | head -15
</span><span class='line'>[
</span><span class='line'>  {
</span><span class='line'>    “uri”: “http://www.ageme.com/AprilAPI/users/savage.evan@gmail.com/documents/2371973/results/76647”,
</span><span class='line'>    “status”: “done”,
</span><span class='line'>    “sequenceType”: “Max72”,
</span><span class='line'>    “sequences”: [
</span><span class='line'>      {
</span><span class='line'>        “smoking”: 0.0,
</span><span class='line'>        “sunExposure”: 0.0,
</span><span class='line'>        “multiplier”: 1.0,
</span><span class='line'>        “images”: [
</span><span class='line'>          {
</span><span class='line'>            “age”: 26,
</span><span class='line'>            “uri”: “http://www.ageme.com/AprilAPI/images/IhLAo8Sp”
</span><span class='line'>          },</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Finally, I wrote a bit of <a href="https://github.com/candu/quantified-savagery-files/blob/master/Aging/fetch_aging.py">Python glue</a> to fetch the URLs and name
them by age:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='py'><span class='line'><span class="kn">import</span> <span class="nn">json</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">sys</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">urllib2</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
</span><span class='line'><span class="n">images</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="err">‘</span><span class="n">sequences</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="err">‘</span><span class="n">images</span><span class="err">’</span><span class="p">]</span>
</span><span class='line'><span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
</span><span class='line'>  <span class="n">url</span> <span class="o">=</span> <span class="n">urllib2</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="err">‘</span><span class="n">uri</span><span class="err">’</span><span class="p">])</span>
</span><span class='line'>  <span class="n">path</span> <span class="o">=</span> <span class="err">‘</span><span class="n">age</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="o">.</span><span class="n">jpg</span><span class="err">’</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="err">‘</span><span class="n">age</span><span class="err">’</span><span class="p">])</span>
</span><span class='line'>  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="err">‘</span><span class="n">w</span><span class="err">’</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span class='line'>    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">url</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>With this, we can fetch the images:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ python fetch_aging.py &lt; aging_results.json</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>And that’s it! Most of the process uses <code>curl</code>, with minimal leaning
on Python for its <code>base64</code> module.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[datafist: Exploration And Analysis]]></title>
    <link href="http://blog.savageevan.com/blog/2012/12/21/datafist-exploration-and-analysis/"/>
    <updated>2012-12-21T07:00:00-08:00</updated>
    <id>http://blog.savageevan.com/blog/2012/12/21/datafist-exploration-and-analysis</id>
    <content type="html"><![CDATA[<p>In this post I introduce datafist, an in-browser tool for visually exploring
your data. </p>

<!-- more -->

<h2 id="first-a-quote">First, A Quote</h2>

<p><blockquote><p>Are we analyzing data? Then we should be manipulating the data themselves; or if we are designing an analysis of data, we should be manipulating the analytic structures themselves.</p><footer><strong>Direct Manipulation Interfaces</strong> <cite><a href='http://cleo.ics.uci.edu/teaching/Winter10/231/readings/1-HutchinsHollanNorman-DirectManipulation-HCI.pdf'>cleo.ics.uci.edu/teaching/231/&hellip;</a></cite></footer></blockquote></p>

<h2 id="the-sixfold-path-of-self-tracking">The Sixfold Path Of Self-Tracking</h2>

<p>Self-tracking is a complex process. It can be broken down into stages:</p>

<ul>
  <li><em>Intent:</em> our initial goals or motivations in deciding to self-track.</li>
  <li><em>Tools:</em> the devices or methods we use for tracking.</li>
  <li><em>Measurement:</em> collecting our data using those tools.</li>
  <li><em>Analysis:</em> extracting insights from our data.</li>
  <li><em>Interpretation:</em> creating personal meaning from those insights.</li>
  <li><em>Action:</em> responding to that personal meaning.</li>
</ul>

<p>Thanks to smartphones and cheap sensors, many of us already have the tools
necessary for measurement. The psychology of intent and action is rapidly
being explored through <a href="http://www.meetup.com/habitdesign/">Habit Design</a> and <a href="http://captology.stanford.edu/">Captology</a>, with
startups like <a href="http://lift.do/">Lift</a> and <a href="https://www.beeminder.com/">Beeminder</a> harnessing the findings
to great effect.</p>

<p><em>What are the technologies of analysis and interpretation?</em></p>

<h3 id="visualization">Visualization</h3>

<p>Visualizations, even interactive ones, are usually designed to <em>answer
specific questions</em> such as <a href="http://www.nytimes.com/interactive/2012/11/07/us/politics/obamas-diverse-base-of-support.html">how did Obama win re-election?</a>
Within the context of those questions, they help us understand our data
intuitively. Outside that context, however, they are often useless.</p>

<h3 id="statisticalmathematical-software">Statistical/Mathematical Software</h3>

<p>Environments like <a href="http://www.r-project.org/">R</a> and <a href="http://www.wolfram.com/mathematica/">Mathematica</a> allow you to <em>explore
your data in meticulous detail.</em> In the hands of people like
<a href="http://blog.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/">Stephen Wolfram</a>, they are the holy grail of data analysis. For the
less technically inclined, they remain hopelessly unintuitive.</p>

<h2 id="a-middle-road">A Middle Road</h2>

<p>What we need is <em>something between the two</em>, a hybrid that exposes the
exploratory power of the latter through the intuitive interface of the former.
Such a tool would give us the opportunity to <em>explore our data as we see fit.</em>
We could ask our data questions, iterating quickly on those questions until
we reach useful insights.</p>

<p>Until we can all converse with our data with the fluency of <a href="http://www.ted.com/talks/hans_rosling_the_good_news_of_the_decade.html">Hans Rosling</a>,
there’s still room for improvement!</p>

<h2 id="datafist">datafist</h2>

<p>datafist tries to bridge this gap by providing <em>visual and gestural actions</em>
for data manipulation. This is probably easier to demonstrate than describe,
so here’s a screencast that shows an early development version of datafist
in action:</p>

<div>
  <iframe width="560" height="315" src="http://www.youtube.com/embed/ypitHPXKa8M" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>As you use datafist, <em>you’re constantly modifying the analysis itself.</em>
This modification takes place through <em>visual and gestural actions:</em> you
move channels to the viewer, drag out ranges of time to zoom in on, and draw
regions around interesting clusters. As you do this, the view is
<em>updated in real-time</em>, allowing you to see the effects of your actions.</p>

<h3 id="try-datafist-out">Try datafist Out!</h3>

<p>I’m hosting a version of datafist <a href="http://datafist.savageevan.com">here at savageevan.com</a>.
Note that this is still a very early development version!</p>

<h3 id="contribute-to-datafist">Contribute to datafist!</h3>

<p>If you’re interested in making datafist better, <a href="https://github.com/candu/datafist">fork me on github!</a>
Bug reports should be submitted <a href="https://github.com/candu/datafist/issues">via the issue tracker</a>.
In particular, if you have a CSV file that won’t import properly, please attach
it for testing purposes!</p>

<h2 id="inspiration">Inspiration</h2>

<ul>
  <li><a href="http://www.audiomulch.com/">AudioMulch</a> is an awesome graphical audio synthesis tool.</li>
  <li><a href="http://puredata.info/">PureData</a> and <a href="http://cycling74.com/products/max/">Max/MSP</a> are visual signal processing languages.</li>
  <li><a href="http://cleo.ics.uci.edu/teaching/Winter10/231/readings/1-HutchinsHollanNorman-DirectManipulation-HCI.pdf">Direct Manipulation Interfaces</a> is a classic paper on the design of
natural-seeming interfaces. You’ll recognize some of the interface concepts
from the analysis package design mocks at the beginning.</li>
  <li><a href="http://www.startuplessonslearned.com/2011/11/startup-is-vision.html">FAKEGRIMLOCK</a> is a fountain of poorly-Englished wisdom on
entrepreneurship.</li>
</ul>
]]></content>
  </entry>
  
</feed>
